# How the Document QA Application Works

## ğŸ¯ Quick Overview

This is a **Retrieval-Augmented Generation (RAG)** system that answers questions based on your documents.

## ğŸ“Š Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documents  â”‚ (Text files in data/sample_docs/)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ingestion     â”‚ â†’ Extract text â†’ Create embeddings
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Weaviate   â”‚ (Vector database storing embeddings)
â”‚  Vector DB  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Query Flow:
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Question                              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€ Baseline Mode â”€â”€â”€â”€â”
       â”‚                       â”‚
       â”‚   Embed Query         â”‚
       â”‚   Directly            â”‚
       â”‚                       â”‚
       â””â”€â”€â”€ HyDE Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    Generate Hypothetical Answer
                               â”‚
                    Embed Hypothetical Doc
                               â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector     â”‚ â†’ Find similar documents (top K)
â”‚  Search     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Retrieved  â”‚ â†’ Top 3 most relevant documents
â”‚  Documents  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LLM     â”‚ â†’ Generate answer from context
â”‚  (MockLLM)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Answer    â”‚ + Source documents
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Step-by-Step Process

### 1. **Document Ingestion** (Happens at startup)
- Reads text files from `data/sample_docs/`
- Splits documents into chunks
- Converts text to vectors using `sentence-transformers/all-MiniLM-L6-v2`
- Stores vectors in Weaviate database

### 2. **Query Processing - Baseline Mode**
```
User Question
    â†“
Embed question directly
    â†“
Search Weaviate for similar vectors
    â†“
Retrieve top 3 documents
    â†“
LLM generates answer using retrieved context
```

### 3. **Query Processing - HyDE Mode** (Enhanced)
```
User Question
    â†“
Generate hypothetical answer document (using LLM)
    â†“
Embed the hypothetical document (not the question!)
    â†“
Search Weaviate for similar vectors
    â†“
Retrieve top 3 documents
    â†“
LLM generates final answer using retrieved context
```

**Why HyDE?** By generating a hypothetical answer first, we get a better embedding that matches the style/content of actual documents, improving retrieval accuracy.

## ğŸ› ï¸ Components

| Component | Purpose |
|-----------|---------|
| **Weaviate** | Vector database storing document embeddings |
| **Sentence-Transformers** | Converts text to 384-dimensional vectors |
| **MockLLM** | Generates answers from retrieved context (simplified for demo) |
| **FastAPI** | REST API server handling requests |
| **HyDE** | Hypothetical Document Embeddings for improved retrieval |

## ğŸ“¡ API Endpoints

### 1. Health Check
```bash
GET http://localhost:8000/health
```
Returns: `{"status": "ok"}`

### 2. Query Documents
```bash
POST http://localhost:8000/query
Content-Type: application/json

{
  "question": "Your question here",
  "mode": "baseline"  // or "hyde"
}
```

Returns:
```json
{
  "answer": "Generated answer...",
  "sources": [
    {
      "source": "filename.txt",
      "title": "title",
      "content": "Retrieved document content..."
    }
  ]
}
```

### 3. Ingest Documents
```bash
POST http://localhost:8000/ingest
```
Re-ingests documents from `data/sample_docs/`

## ğŸš€ How to Use

### Method 1: PowerShell
```powershell
$body = @{
    question = "What does ingestion do?"
    mode = "baseline"
} | ConvertTo-Json

$result = Invoke-RestMethod -Uri "http://localhost:8000/query" `
    -Method POST -Body $body -ContentType "application/json"

Write-Host $result.answer
```

### Method 2: Browser (Interactive)
1. Open: http://localhost:8000/docs
2. Click on `POST /query`
3. Click "Try it out"
4. Enter your question and mode
5. Click "Execute"

### Method 3: curl
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question":"Your question","mode":"baseline"}'
```

## ğŸ” Example Output

**Question:** "What does ingestion do?"

**Response:**
```json
{
  "answer": "Based on the retrieved documents, ingestion reads text files from the data directory and writes embeddings to Weaviate.",
  "sources": [
    {
      "source": "process.txt",
      "title": "process",
      "content": "Ingestion reads text files from the data directory and writes embeddings to Weaviate..."
    }
  ]
}
```

## ğŸ“ˆ Key Differences: Baseline vs HyDE

| Aspect | Baseline | HyDE |
|--------|----------|------|
| Query Embedding | Direct question embedding | Hypothetical document embedding |
| Retrieval Quality | Standard | Enhanced (better semantic matching) |
| Processing Time | Faster | Slightly slower (extra LLM call) |
| Use Case | General queries | Complex/ambiguous questions |

## ğŸ“ Learning Points

1. **Vector Search**: Documents are stored as vectors, allowing semantic similarity search
2. **Retrieval-Augmented Generation**: Answers are generated from retrieved context, not just LLM knowledge
3. **HyDE Enhancement**: Generating hypothetical documents improves retrieval by matching document style
4. **Local LLM**: Uses MockLLM for demonstration (can be replaced with GPT4All or other LLMs)

## ğŸ”§ Current Status

- âœ… Weaviate running on port 8080
- âœ… FastAPI running on port 8000
- âœ… Documents ingested and searchable
- âœ… Baseline and HyDE modes working
- âœ… API endpoints functional

Visit **http://localhost:8000/docs** for interactive API testing!


